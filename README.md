# CAMELYON+ BENCHMARK
## INTRODUCTION
### *why we do this work?*
Multiple Instance Learning **_(MIL)_** methods are mainstream approaches for pathological image classification and analysis.
The ***CAMELYON-16/17*** datasets are commonly used to evaluate ***MIL*** methods. 
However, they have the following issues:
* **_CAMELYON-16/17_** datasets contain some problematic slides
* Pixel-annotations of **_CAMELYON-16/17_** test-dataset not accurate enough
* Different **_MIL_** methods do not have a unified dataset-split and evaluation-metrics on the ***CAMELYON*** dataset
* To conclude,there is no ***BENCHMARK*** for ***MIL*** methods
### *what we do in this work?*
We do the following work to establish a ***CAMELYON+ BENCHMARK***
* Remove some problematic slides.
* Correct problematic annotations.
* Merge the correct version of**_CAMELYON-16/17_** datasets as the **_CAMELYON+_** dataset.
* Evaluate mainstream ***MIL*** methods on the **_CAMELYON-NEW_** dataset.
* Evaluate mainstream feature extractors on the **_CAMELYON-NEW_** dataset.
* Use more comprehensive evaluation metrics to assess different methods.
* In summary, we establish a new **_CAMELYON+ BENCHMARK_**.

## CAMELYON+
### *arxiv link*
* [ARXIV](https://arxiv.org/pdf/2411.10752)
### *dataset download*
* [SCIENCEDB](https://www.scidb.cn/en/detail?dataSetId=cc1f911b75ca4610bd02ac33a51898a9)
### *code available*
* [MIL_BASELINE](https://github.com/lingxitong/MIL_BASELINE)


